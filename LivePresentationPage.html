<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Live Presentation | AI Presentation Analyzer</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
  <style>
    :root {
      --gold: #d4af37;
      --gold-dark: #b38f2d;
      --black: #0c0c0c;
      --white: #ffffff;
    }

    body {
      background: linear-gradient(135deg, var(--black), #1a1a1a);
      color: var(--white);
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }

    .navbar {
      background: var(--black);
      border-bottom: 2px solid var(--gold);
      box-shadow: 0 2px 10px rgba(212,175,55,0.2);
    }

    .navbar-brand {
      display: flex;
      align-items: center;
      color: var(--gold);
      font-weight: 700;
      font-size: 1.2rem;
    }

    .navbar-brand img {
      height: 65px;
      margin-right: 12px;
    }

    .timer-display {
      font-size: 18px;
      font-weight: 600;
      color: var(--gold);
      background: #1b1b1b;
      padding: 6px 12px;
      border-radius: 8px;
      border: 1px solid var(--gold);
    }

    .btn-record {
      background: var(--gold);
      color: var(--black);
      border: none;
      font-weight: 600;
    }

    .btn-record:hover {
      background: var(--gold-dark);
      color: var(--white);
    }

    .btn-pause {
      background: transparent;
      border: 1px solid var(--gold);
      color: var(--gold);
      font-weight: 600;
    }

    .btn-pause:hover {
      background: var(--gold);
      color: var(--black);
    }

    .btn-danger {
      background: #dc3545;
      color: white;
      border: none;
      font-weight: 600;
    }

    .btn-danger:hover {
      background: #c82333;
      color: white;
    }

    .metrics-card {
      background: #141414;
      border-radius: 12px;
      padding: 20px;
      border: 1px solid rgba(212,175,55,0.3);
      box-shadow: 0 0 12px rgba(212,175,55,0.1);
      color: var(--white);
      transition: transform 0.3s;
    }

    .metrics-card:hover {
      transform: translateY(-3px);
    }

    .metrics-card h5 {
      color: var(--gold);
      border-bottom: 1px solid rgba(212,175,55,0.3);
      padding-bottom: 10px;
      margin-bottom: 15px;
    }

    .video-container {
      background: #000;
      border-radius: 12px;
      overflow: hidden;
      box-shadow: 0 0 15px rgba(212,175,55,0.2);
      position: relative;
    }

    .video-overlay {
      position: absolute;
      bottom: 0;
      left: 0;
      right: 0;
      background: rgba(0,0,0,0.7);
      color: var(--white);
      padding: 10px 15px;
      display: flex;
      justify-content: space-between;
      align-items: center;
      font-size: 14px;
    }

    .live-indicator {
      width: 10px;
      height: 10px;
      background: var(--gold);
      border-radius: 50%;
      margin-right: 8px;
      animation: pulse 1.5s infinite;
    }

    @keyframes pulse {
      0% { opacity: 1; }
      50% { opacity: 0.5; }
      100% { opacity: 1; }
    }

    .transcript-container {
      max-height: 200px;
      overflow-y: auto;
      padding: 10px;
      background: #1c1c1c;
      border-radius: 8px;
      font-size: 14px;
      color: #e6e6e6;
    }

    .transcript-line {
      border-bottom: 1px solid rgba(255,255,255,0.1);
      margin-bottom: 10px;
      padding-bottom: 8px;
    }

    .transcript-time {
      color: var(--gold);
      font-weight: 600;
    }

    .filler-word {
      background: rgba(212,175,55,0.2);
      padding: 2px 5px;
      border-radius: 4px;
      font-weight: 600;
      color: var(--gold);
    }

    .score-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 15px;
      margin-top: 15px;
    }

    .score-card {
      background: rgba(212,175,55,0.1);
      border-radius: 8px;
      padding: 15px;
      text-align: center;
      border: 1px solid rgba(212,175,55,0.3);
    }

    .score-number {
      font-size: 28px;
      font-weight: 700;
      color: var(--gold);
      margin-bottom: 5px;
    }

    .score-label {
      font-size: 12px;
      color: #ccc;
      text-transform: uppercase;
      letter-spacing: 1px;
    }

    .loading-spinner {
      display: none;
      text-align: center;
      padding: 20px;
    }

    .video-analysis-results {
      margin-top: 15px;
    }

    .video-metric {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin: 8px 0;
      padding: 8px;
      background: rgba(255,255,255,0.05);
      border-radius: 6px;
    }

    .video-metric-label {
      flex: 1;
      font-size: 14px;
    }

    .video-metric-value {
      font-weight: 600;
      color: var(--gold);
      margin: 0 10px;
    }

    .video-metric-bar {
      width: 100px;
      height: 6px;
      background: #333;
      border-radius: 3px;
      overflow: hidden;
    }

    .video-metric-fill {
      height: 100%;
      background: var(--gold);
      transition: width 0.3s ease;
    }

    .upload-section {
      background: rgba(212,175,55,0.1);
      border: 2px dashed var(--gold);
      border-radius: 8px;
      padding: 20px;
      text-align: center;
      margin-top: 15px;
    }

    .debug-info {
      background: rgba(0,0,0,0.5);
      border-radius: 5px;
      padding: 10px;
      font-size: 12px;
      margin-top: 10px;
    }

    .emotion-badge {
      background: rgba(212,175,55,0.2);
      color: var(--gold);
      padding: 4px 8px;
      border-radius: 12px;
      font-size: 12px;
      font-weight: 600;
    }

    /* Report Styles */
    .report-header {
      border-bottom: 2px solid var(--gold);
      padding-bottom: 15px;
      margin-bottom: 20px;
    }

    .suggestions-container {
      max-height: 400px;
      overflow-y: auto;
    }

    .suggestion-item {
      display: flex;
      align-items: flex-start;
      padding: 12px;
      margin-bottom: 10px;
      background: rgba(212,175,55,0.1);
      border-radius: 8px;
      border-left: 4px solid var(--gold);
    }

    .suggestion-icon {
      margin-right: 12px;
      font-size: 1.2em;
    }

    .suggestion-text {
      flex: 1;
    }

    .transcript-preview {
      background: #1c1c1c;
      padding: 15px;
      border-radius: 8px;
      font-size: 14px;
      line-height: 1.5;
      max-height: 200px;
      overflow-y: auto;
    }

    .recording-timer {
      font-size: 24px;
      font-weight: bold;
      color: var(--gold);
      text-align: center;
      margin: 10px 0;
    }

    @media (max-width: 992px) {
      .navbar-brand img {
        height: 55px;
      }
      .score-grid {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>
<body>

<!-- Navbar -->
<nav class="navbar navbar-expand-lg">
  <div class="container">
    <a class="navbar-brand" href="#">
      <img src="logo.png" alt="Logo"> AI Presentation Analyzer
    </a>
    <div class="d-flex align-items-center">
      <div class="timer-display me-3">
        <i class="bi bi-clock me-1"></i> <span id="recordingTimer">00:00</span>
      </div>
      <div class="action-buttons">
        <button class="btn btn-pause" id="startRecordingButton">
          <i class="bi bi-record-circle"></i> Start Recording
        </button>
        <button class="btn btn-record" id="testApiButton">
          <i class="bi bi-play-circle"></i> Test API
        </button>
      </div>
    </div>
  </div>
</nav>

<!-- Main Section -->
<div class="container-fluid mt-4">
  <div class="row g-4">
    <!-- Left Column -->
    <div class="col-lg-6">
      <!-- Video Recording Card -->
      <div class="metrics-card">
        <h5><i class="bi bi-camera-video"></i> Live Recording <span class="live-indicator"></span> <span id="recordingStatus">Ready</span></h5>
        <div class="video-container">
          <video id="liveVideo" autoplay muted playsinline style="width: 100%; height: 350px; object-fit: cover; display: none;"></video>
          <div style="height: 350px; display: flex; justify-content: center; align-items: center; color: var(--white);" id="videoPlaceholder">
            <div class="text-center">
              <i class="bi bi-camera-video" style="font-size: 48px; color: var(--gold);"></i>
              <p>Click "Start Recording" to begin analysis</p>
            </div>
          </div>
          <div class="video-overlay">
            <div id="statusText"><span style="color: #ccc; font-weight: 600;">Ready</span> ‚Ä¢ Start recording for face & voice analysis</div>
            <div id="qualityIndicator"><span style="color: #ccc;">HD</span></div>
          </div>
        </div>

        <!-- Recording Timer -->
        <div class="recording-timer" id="recordingTimerDisplay" style="display: none;">
          <i class="bi bi-stopwatch"></i> <span id="timerDisplay">00:00</span>
        </div>

        <!-- Audio Upload Section -->
        <div class="upload-section">
          <h6><i class="bi bi-mic"></i> Audio Analysis</h6>
          <p class="small text-muted mb-3">Upload your presentation audio for analysis</p>
          <input type="file" id="audioFileInput" accept="audio/*" class="form-control mb-2">
          <button class="btn btn-record" id="analyzeAudioButton">
            <i class="bi bi-graph-up"></i> Analyze Audio
          </button>
          <div class="debug-info" id="audioDebugInfo">
            No file selected
          </div>
        </div>
      </div>

      <!-- Facial Expression Analysis Card -->
      <div class="metrics-card">
        <h5><i class="bi bi-camera-reels"></i> Live Facial Expression Analysis</h5>
        <div class="loading-spinner" id="videoLoadingSpinner">
          <div class="spinner-border text-gold" role="status">
            <span class="visually-hidden">Loading...</span>
          </div>
          <p class="mt-2">Analyzing facial expressions...</p>
        </div>
        <div class="video-analysis-results" id="videoAnalysisResults">
          <div class="video-metric">
            <div class="video-metric-label">üòä Smile Intensity</div>
            <div class="video-metric-value" id="smileIntensityValue">--%</div>
            <div class="video-metric-bar">
              <div class="video-metric-fill" id="smileIntensityBar" style="width: 0%"></div>
            </div>
          </div>
          <div class="video-metric">
            <div class="video-metric-label">üò¨ Nervousness</div>
            <div class="video-metric-value" id="nervousnessValue">--%</div>
            <div class="video-metric-bar">
              <div class="video-metric-fill" id="nervousnessBar" style="width: 0%"></div>
            </div>
          </div>
          <div class="video-metric">
            <div class="video-metric-label">üí™ Confidence</div>
            <div class="video-metric-value" id="confidenceValue">--%</div>
            <div class="video-metric-bar">
              <div class="video-metric-fill" id="confidenceBar" style="width: 0%"></div>
            </div>
          </div>
          <div class="video-metric">
            <div class="video-metric-label">üéØ Engagement</div>
            <div class="video-metric-value" id="engagementValue">--%</div>
            <div class="video-metric-bar">
              <div class="video-metric-fill" id="engagementBar" style="width: 0%"></div>
            </div>
          </div>
          <div class="mt-3 p-2" style="background: rgba(212,175,55,0.1); border-radius: 8px;">
            <small class="text-gold">Dominant Emotion:</small>
            <div class="fw-bold" id="dominantEmotion">--</div>
          </div>
          <div class="debug-info" id="videoDebugInfo">
            Click "Start Recording" to begin analysis
          </div>
        </div>
      </div>
    </div>

    <!-- Right Column -->
    <div class="col-lg-6">
      <!-- Live Transcript Card -->
      <div class="metrics-card">
        <h5><i class="bi bi-chat-left-dots"></i> Transcript</h5>
        <div class="transcript-container" id="transcriptContainer">
          <div class="text-center text-muted py-4">
            <i class="bi bi-mic" style="font-size: 2rem;"></i>
            <p class="mt-2">Start recording or upload audio to see transcript</p>
          </div>
        </div>
      </div>

      <!-- Analysis Results Card -->
      <div class="metrics-card">
        <h5><i class="bi bi-graph-up"></i> Speech Analysis Results</h5>
        <div class="loading-spinner" id="loadingSpinner">
          <div class="spinner-border text-gold" role="status">
            <span class="visually-hidden">Loading...</span>
          </div>
          <p class="mt-2">Analyzing your presentation...</p>
        </div>
        <div class="score-grid" id="scoreGrid">
          <div class="score-card">
            <div class="score-number" id="overallScore">--%</div>
            <div class="score-label">Overall Score</div>
          </div>
          <div class="score-card">
            <div class="score-number" id="clarityScore">--%</div>
            <div class="score-label">Clarity</div>
          </div>
          <div class="score-card">
            <div class="score-number" id="paceScore">--%</div>
            <div class="score-label">Pace</div>
          </div>
          <div class="score-card">
            <div class="score-number" id="prosodyScore">--%</div>
            <div class="score-label">Prosody</div>
          </div>
        </div>
        
        <!-- Detailed Metrics -->
        <div class="mt-4" id="detailedMetrics" style="display: none;">
          <h6 class="text-gold mb-3">Detailed Metrics</h6>
          <div class="row">
            <div class="col-6">
              <small class="text-muted">Words Spoken:</small>
              <div class="fw-bold" id="wordCount">--</div>
            </div>
            <div class="col-6">
              <small class="text-muted">Filler Words:</small>
              <div class="fw-bold" id="fillerCount">--</div>
            </div>
            <div class="col-6">
              <small class="text-muted">Speaking Rate:</small>
              <div class="fw-bold" id="speakingRate">-- WPM</div>
            </div>
            <div class="col-6">
              <small class="text-muted">Sentiment:</small>
              <div class="fw-bold" id="sentimentScore">--</div>
            </div>
          </div>
        </div>
        <div class="debug-info" id="audioAnalysisDebugInfo">
          Audio analysis not performed yet
        </div>
      </div>

      <!-- Comprehensive Report Section -->
      <div class="metrics-card" id="reportSection" style="display: none;">
        <h5><i class="bi bi-clipboard-data"></i> Presentation Analysis Report</h5>
        <div id="reportContent">
          <!-- Report will be generated here -->
        </div>
      </div>
    </div>
  </div>
</div>

<!-- Footer -->
<footer>
  <p>¬© <span>2025 AI Presentation Analyzer</span>. All rights reserved.</p>
</footer>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
<script>
  // ========== Configuration ==========
  const API_BASE_URL = 'http://localhost:5000';

  // ========== Global Variables ==========
  let videoStream = null;
  let audioStream = null;
  let audioRecorder = null;
  let frameAnalysisInterval;
  let recordingTimerInterval;
  let audioChunks = [];
  let frameCount = 0;
  let recordingStartTime = 0;
  let isRecording = false;
  let isCameraActive = false;

  // Data storage for analysis
  let facialExpressionData = [];
  let voiceAnalysisData = null;
  let videoAnalysisData = {
    totalFrames: 0,
    framesWithFace: 0,
    averageSmile: 0,
    averageNervousness: 0,
    averageConfidence: 0,
    averageEngagement: 0,
    emotionDistribution: {}
  };

  // DOM Elements
  const startRecordingButton = document.getElementById('startRecordingButton');
  const testApiButton = document.getElementById('testApiButton');
  const analyzeAudioButton = document.getElementById('analyzeAudioButton');
  const audioFileInput = document.getElementById('audioFileInput');
  const transcriptContainer = document.getElementById('transcriptContainer');
  const recordingStatus = document.getElementById('recordingStatus');
  const statusText = document.getElementById('statusText');
  const qualityIndicator = document.getElementById('qualityIndicator');
  const loadingSpinner = document.getElementById('loadingSpinner');
  const videoLoadingSpinner = document.getElementById('videoLoadingSpinner');
  const scoreGrid = document.getElementById('scoreGrid');
  const detailedMetrics = document.getElementById('detailedMetrics');
  const videoPlaceholder = document.getElementById('videoPlaceholder');
  const liveVideo = document.getElementById('liveVideo');
  const videoAnalysisResults = document.getElementById('videoAnalysisResults');
  const recordingTimerDisplay = document.getElementById('recordingTimerDisplay');
  const timerDisplay = document.getElementById('timerDisplay');

  // Debug Elements
  const audioDebugInfo = document.getElementById('audioDebugInfo');
  const videoDebugInfo = document.getElementById('videoDebugInfo');
  const audioAnalysisDebugInfo = document.getElementById('audioAnalysisDebugInfo');

  // Video Analysis DOM Elements
  const smileIntensityValue = document.getElementById('smileIntensityValue');
  const smileIntensityBar = document.getElementById('smileIntensityBar');
  const nervousnessValue = document.getElementById('nervousnessValue');
  const nervousnessBar = document.getElementById('nervousnessBar');
  const confidenceValue = document.getElementById('confidenceValue');
  const confidenceBar = document.getElementById('confidenceBar');
  const engagementValue = document.getElementById('engagementValue');
  const engagementBar = document.getElementById('engagementBar');
  const dominantEmotion = document.getElementById('dominantEmotion');

  // ========== MediaRecorder Utilities ==========
  function getSupportedMimeType() {
    const types = [
      'audio/webm;codecs=opus',
      'audio/webm',
      'audio/mp4;codecs=mp4a',
      'audio/mp4',
      'audio/ogg;codecs=opus',
      'audio/wav',
      'audio/aac',
      '' // Empty string lets browser choose default
    ];

    for (let type of types) {
      if (type === '' || MediaRecorder.isTypeSupported(type)) {
        console.log("üéµ Using MIME type:", type || 'browser-default');
        return type;
      }
    }
    
    console.warn("‚ö†Ô∏è No specific MIME type supported, using default");
    return '';
  }

  function createMediaRecorder(stream) {
    const mimeType = getSupportedMimeType();
    const options = mimeType ? { mimeType } : {};
    
    try {
      console.log("üé§ Creating MediaRecorder with options:", options);
      
      // Check if stream has audio tracks
      const audioTracks = stream.getAudioTracks();
      console.log("üéµ Audio tracks available:", audioTracks.length);
      if (audioTracks.length === 0) {
        throw new Error('No audio tracks in stream');
      }
      
      // Check track status
      audioTracks.forEach((track, index) => {
        console.log(`üéµ Track ${index}:`, {
          kind: track.kind,
          label: track.label,
          readyState: track.readyState,
          enabled: track.enabled,
          muted: track.muted
        });
      });

      const recorder = new MediaRecorder(stream, options);
      
      // Test if we can start the recorder
      return new Promise((resolve, reject) => {
        const testChunks = [];
        recorder.ondataavailable = (event) => {
          if (event.data && event.data.size > 0) {
            testChunks.push(event.data);
          }
        };
        
        recorder.onstart = () => {
          console.log("‚úÖ MediaRecorder started successfully");
          // Stop immediately after successful start
          setTimeout(() => {
            if (recorder.state === 'recording') {
              recorder.stop();
            }
          }, 100);
        };
        
        recorder.onstop = () => {
          console.log("‚úÖ MediaRecorder test completed");
          resolve(recorder);
        };
        
        recorder.onerror = (event) => {
          console.error("‚ùå MediaRecorder test failed:", event.error);
          reject(new Error(`MediaRecorder failed: ${event.error}`));
        };
        
        // Start with minimal timeslice
        try {
          recorder.start(10);
        } catch (startError) {
          reject(new Error(`Cannot start MediaRecorder: ${startError.message}`));
        }
      });
      
    } catch (error) {
      throw new Error(`MediaRecorder creation failed: ${error.message}`);
    }
  }

  // ========== Enhanced Recording Functions ==========
  async function startCombinedRecording() {
    try {
      console.log("üé¨ Starting combined video and audio recording...");
      
      // Check browser support
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('Your browser does not support camera/microphone access. Please use Chrome, Firefox, or Edge.');
      }

      // Show loading state
      startRecordingButton.disabled = true;
      startRecordingButton.innerHTML = '<i class="bi bi-hourglass"></i> Requesting Access...';

      let stream;
      try {
        // Try with basic constraints first - they're more likely to work
        console.log("üéØ Trying basic media constraints...");
        stream = await navigator.mediaDevices.getUserMedia({ 
          video: {
            width: { ideal: 640 },
            height: { ideal: 480 }
          },
          audio: {
            echoCancellation: true,
            noiseSuppression: true
          }
        });
        console.log("‚úÖ Got stream with basic constraints");
      } catch (basicError) {
        console.warn("‚ö†Ô∏è Basic constraints failed, trying minimal constraints:", basicError);
        
        // Minimal constraints as fallback
        stream = await navigator.mediaDevices.getUserMedia({ 
          video: true,
          audio: true
        });
        console.log("‚úÖ Got stream with minimal constraints");
      }
      
      // Setup video
      await setupVideoStream(stream);
      
      // Setup audio recording with enhanced error handling
      await setupAudioRecording(stream);
      
      // Start analyses and update UI
      startAnalyses();
      
      console.log("‚úÖ Combined recording started successfully");
      
    } catch (error) {
      console.error('‚ùå Error starting recording:', error);
      handleRecordingError(error);
    } finally {
      startRecordingButton.disabled = false;
    }
  }

  function setupVideoStream(stream) {
    videoStream = stream;
    liveVideo.srcObject = videoStream;
    liveVideo.style.display = 'block';
    videoPlaceholder.style.display = 'none';
    
    return new Promise((resolve) => {
      if (liveVideo.readyState >= 2) { // HAVE_CURRENT_DATA
        console.log("üìπ Video stream ready immediately");
        resolve();
      } else {
        liveVideo.onloadedmetadata = () => {
          console.log("üìπ Video stream ready:", liveVideo.videoWidth, "x", liveVideo.videoHeight);
          resolve();
        };
        liveVideo.onerror = () => {
          console.warn("‚ö†Ô∏è Video stream had error, but continuing...");
          resolve();
        };
        // Fallback timeout
        setTimeout(resolve, 1000);
      }
    });
  }

  async function setupAudioRecording(stream) {
    audioStream = stream;
    
    try {
      console.log("üé§ Setting up audio recording...");
      
      // Get audio tracks info
      const audioTracks = stream.getAudioTracks();
      if (audioTracks.length === 0) {
        throw new Error('No audio tracks available in the stream');
      }

      // Test and create MediaRecorder
      audioRecorder = await createMediaRecorder(stream);
      
      // Reset for actual recording
      audioChunks = [];
      audioRecorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          audioChunks.push(event.data);
          console.log("üéµ Audio chunk:", event.data.size, "bytes");
        }
      };
      
      audioRecorder.onstop = processCombinedAnalysis;
      audioRecorder.onerror = (event) => {
        console.error("‚ùå Audio recorder error:", event.error);
        videoDebugInfo.textContent = `Audio error: ${event.error}`;
      };

      // Start the actual recording with a safe timeslice
      console.log("üé§ Starting audio recording...");
      audioRecorder.start(1000); // 1-second chunks
      
      recordingStartTime = Date.now();
      isRecording = true;
      isCameraActive = true;
      
      console.log("‚úÖ Audio recording setup completed");
      
    } catch (error) {
      console.error("‚ùå Audio recording setup failed:", error);
      
      // Fallback: Try direct recording without MIME type
      console.log("üîÑ Trying fallback audio recording...");
      try {
        audioRecorder = new MediaRecorder(stream); // No options
        audioChunks = [];
        audioRecorder.ondataavailable = (event) => {
          if (event.data && event.data.size > 0) {
            audioChunks.push(event.data);
          }
        };
        audioRecorder.onstop = processCombinedAnalysis;
        audioRecorder.start(1000);
        
        recordingStartTime = Date.now();
        isRecording = true;
        isCameraActive = true;
        
        console.log("‚úÖ Fallback audio recording started");
      } catch (fallbackError) {
        throw new Error(`All audio recording attempts failed: ${fallbackError.message}`);
      }
    }
  }

  function startAnalyses() {
    // Reset data
    facialExpressionData = [];
    frameCount = 0;
    videoAnalysisData = {
      totalFrames: 0,
      framesWithFace: 0,
      averageSmile: 0,
      averageNervousness: 0,
      averageConfidence: 0,
      averageEngagement: 0,
      emotionDistribution: {}
    };
    
    // Update UI
    startRecordingButton.innerHTML = '<i class="bi bi-stop-fill"></i> Stop Recording';
    startRecordingButton.classList.remove('btn-pause');
    startRecordingButton.classList.add('btn-danger');
    recordingStatus.textContent = 'Recording Active';
    statusText.innerHTML = '<span style="color: var(--gold); font-weight: 600;">Recording Active</span> ‚Ä¢ Analyzing face & voice';
    qualityIndicator.innerHTML = '<span style="color: var(--gold);">‚óè LIVE</span>';
    recordingTimerDisplay.style.display = 'block';
    
    // Start analyses
    startFrameAnalysis();
    startRecordingTimer();
  }

  function handleRecordingError(error) {
    let errorMessage = 'Could not start recording. ';
    let detailedMessage = error.message;
    
    if (error.name === 'NotAllowedError') {
      errorMessage += 'Please allow camera and microphone permissions. ';
      detailedMessage = 'Permission denied by user';
    } else if (error.name === 'NotFoundError') {
      errorMessage += 'No camera or microphone detected. ';
      detailedMessage = 'No media devices found';
    } else if (error.name === 'NotSupportedError') {
      errorMessage += 'Your browser does not support recording. Try Chrome or Firefox. ';
      detailedMessage = 'Browser not supported';
    } else if (error.message.includes('MediaRecorder')) {
      errorMessage += 'Audio recording not supported. ';
      detailedMessage = 'MediaRecorder API failed';
    } else {
      errorMessage += `Error: ${error.message}`;
    }

    errorMessage += '\n\nTroubleshooting:\n';
    errorMessage += '‚Ä¢ Refresh the page and try again\n';
    errorMessage += '‚Ä¢ Check browser permissions\n';
    errorMessage += '‚Ä¢ Try a different browser (Chrome recommended)\n';
    errorMessage += '‚Ä¢ Ensure no other app is using camera/microphone';

    console.error('‚ùå Recording error:', {
      name: error.name,
      message: error.message,
      detailed: detailedMessage
    });
    
    alert(errorMessage);
    videoDebugInfo.textContent = `Error: ${detailedMessage}`;
  }

  function stopCombinedRecording() {
    if (!isRecording) return;
    
    console.log("üõë Stopping recording...");
    isRecording = false;
    isCameraActive = false;
    
    // Stop audio recording first
    if (audioRecorder && audioRecorder.state === 'recording') {
      console.log("üé§ Stopping audio recorder");
      audioRecorder.stop();
    }
    
    // Stop media tracks
    if (videoStream) {
      videoStream.getTracks().forEach(track => {
        console.log("üìπ Stopping track:", track.kind);
        track.stop();
      });
      videoStream = null;
    }
    
    if (audioStream) {
      audioStream.getTracks().forEach(track => track.stop());
      audioStream = null;
    }
    
    // Stop analyses
    stopFrameAnalysis();
    stopRecordingTimer();
    
    // Update UI
    resetUI();
    
    console.log("‚úÖ Recording stopped completely");
  }

  function resetUI() {
    startRecordingButton.innerHTML = '<i class="bi bi-record-circle"></i> Start Recording';
    startRecordingButton.classList.remove('btn-danger');
    startRecordingButton.classList.add('btn-pause');
    recordingStatus.textContent = 'Ready';
    statusText.innerHTML = '<span style="color: #ccc; font-weight: 600;">Ready</span> ‚Ä¢ Start recording for analysis';
    qualityIndicator.innerHTML = '<span style="color: #ccc;">HD</span>';
    recordingTimerDisplay.style.display = 'none';
    
    // Hide video and show placeholder
    liveVideo.style.display = 'none';
    liveVideo.srcObject = null;
    videoPlaceholder.style.display = 'flex';
    
    // Reset analysis displays
    videoLoadingSpinner.style.display = 'none';
    videoAnalysisResults.style.display = 'block';
    
    smileIntensityValue.textContent = '--%';
    smileIntensityBar.style.width = '0%';
    nervousnessValue.textContent = '--%';
    nervousnessBar.style.width = '0%';
    confidenceValue.textContent = '--%';
    confidenceBar.style.width = '0%';
    engagementValue.textContent = '--%';
    engagementBar.style.width = '0%';
    dominantEmotion.textContent = '--';
    
    videoDebugInfo.textContent = 'Click "Start Recording" to begin analysis';
  }

  // ========== Frame Analysis ==========
  function startFrameAnalysis() {
    console.log("üé¨ Starting frame analysis...");
    videoLoadingSpinner.style.display = 'block';
    videoAnalysisResults.style.display = 'none';
    
    frameAnalysisInterval = setInterval(async () => {
      if (!isCameraActive || !liveVideo.videoWidth) return;
      
      try {
        const canvas = document.createElement('canvas');
        const context = canvas.getContext('2d');
        canvas.width = liveVideo.videoWidth;
        canvas.height = liveVideo.videoHeight;
        context.drawImage(liveVideo, 0, 0, canvas.width, canvas.height);
        
        canvas.toBlob(async (blob) => {
          if (!blob) {
            console.error("‚ùå Failed to create image blob");
            return;
          }
          
          const formData = new FormData();
          formData.append('frame', blob, `frame_${frameCount}.jpg`);
          
          try {
            videoDebugInfo.textContent = `Analyzing frame ${frameCount}...`;
            
            const response = await fetch(`${API_BASE_URL}/api/analyze-frame`, {
              method: 'POST',
              body: formData
            });
            
            if (response.ok) {
              const result = await response.json();
              updateVideoAnalysis(result);
              storeFacialExpressionData(result);
              videoDebugInfo.textContent = `Frame ${frameCount}: ${result.expression_analysis.dominant_emotion}`;
            } else {
              console.error("‚ùå Frame analysis failed");
            }
          } catch (error) {
            console.error('‚ùå Frame analysis request failed:', error);
          }
        }, 'image/jpeg', 0.8);
        
        frameCount++;
      } catch (error) {
        console.error('‚ùå Frame capture error:', error);
      }
    }, 3000);
  }

  function stopFrameAnalysis() {
    if (frameAnalysisInterval) {
      clearInterval(frameAnalysisInterval);
      console.log("üõë Frame analysis stopped");
    }
  }

  function storeFacialExpressionData(result) {
    if (result.expression_analysis && result.face_detected) {
      const expression = result.expression_analysis;
      
      facialExpressionData.push({
        timestamp: Date.now() - recordingStartTime,
        smile: expression.smile_intensity,
        nervousness: expression.nervousness,
        confidence: expression.confidence,
        engagement: expression.engagement,
        dominantEmotion: expression.dominant_emotion,
        emotions: expression.emotions
      });
      
      videoAnalysisData.totalFrames++;
      videoAnalysisData.framesWithFace++;
      
      videoAnalysisData.averageSmile = ((videoAnalysisData.averageSmile * (videoAnalysisData.framesWithFace - 1)) + expression.smile_intensity) / videoAnalysisData.framesWithFace;
      videoAnalysisData.averageNervousness = ((videoAnalysisData.averageNervousness * (videoAnalysisData.framesWithFace - 1)) + expression.nervousness) / videoAnalysisData.framesWithFace;
      videoAnalysisData.averageConfidence = ((videoAnalysisData.averageConfidence * (videoAnalysisData.framesWithFace - 1)) + expression.confidence) / videoAnalysisData.framesWithFace;
      videoAnalysisData.averageEngagement = ((videoAnalysisData.averageEngagement * (videoAnalysisData.framesWithFace - 1)) + expression.engagement) / videoAnalysisData.framesWithFace;
      
      if (!videoAnalysisData.emotionDistribution[expression.dominant_emotion]) {
        videoAnalysisData.emotionDistribution[expression.dominant_emotion] = 0;
      }
      videoAnalysisData.emotionDistribution[expression.dominant_emotion]++;
    }
  }

  function updateVideoAnalysis(result) {
    videoLoadingSpinner.style.display = 'none';
    videoAnalysisResults.style.display = 'block';
    
    if (result.expression_analysis) {
      const exp = result.expression_analysis;
      
      smileIntensityValue.textContent = Math.round(exp.smile_intensity) + '%';
      smileIntensityBar.style.width = exp.smile_intensity + '%';
      
      nervousnessValue.textContent = Math.round(exp.nervousness) + '%';
      nervousnessBar.style.width = exp.nervousness + '%';
      
      confidenceValue.textContent = Math.round(exp.confidence) + '%';
      confidenceBar.style.width = exp.confidence + '%';
      
      engagementValue.textContent = Math.round(exp.engagement) + '%';
      engagementBar.style.width = exp.engagement + '%';
      
      dominantEmotion.textContent = exp.dominant_emotion.charAt(0).toUpperCase() + exp.dominant_emotion.slice(1);
      
      videoDebugInfo.textContent = `Live: ${exp.dominant_emotion}, Smile: ${Math.round(exp.smile_intensity)}%`;
    }
  }

  // ========== Timer Functions ==========
  function startRecordingTimer() {
    recordingTimerInterval = setInterval(() => {
      const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000);
      const minutes = Math.floor(elapsed / 60);
      const seconds = elapsed % 60;
      timerDisplay.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    }, 1000);
  }

  function stopRecordingTimer() {
    if (recordingTimerInterval) {
      clearInterval(recordingTimerInterval);
    }
  }

  // ========== Audio Processing ==========
  async function processCombinedAnalysis() {
    console.log("üìä Processing combined analysis...");
    
    try {
      if (audioChunks.length === 0) {
        throw new Error('No audio data recorded');
      }

      const audioBlob = new Blob(audioChunks);
      const recordingDuration = (Date.now() - recordingStartTime) / 1000;
      
      console.log("üéµ Audio recording completed:", {
        duration: recordingDuration,
        chunks: audioChunks.length,
        totalSize: audioBlob.size
      });
      
      videoDebugInfo.textContent = 'Generating comprehensive report...';
      await analyzeRecordedAudio(audioBlob, recordingDuration);
      
    } catch (error) {
      console.error('‚ùå Error processing combined analysis:', error);
      videoDebugInfo.textContent = `Error: ${error.message}`;
    }
  }

  async function analyzeRecordedAudio(audioBlob, duration) {
    const formData = new FormData();
    formData.append('audio', audioBlob, 'presentation_recording.webm');
    formData.append('duration', duration.toString());
    
    try {
      console.log("üì§ Sending live audio for analysis...");
      const response = await fetch(`${API_BASE_URL}/api/analyze-live-audio`, {
        method: 'POST',
        body: formData
      });
      
      if (!response.ok) {
        throw new Error(`Server error: ${response.status}`);
      }
      
      const data = await response.json();
      console.log("‚úÖ Voice analysis received");
      
      voiceAnalysisData = {
        ...data.analysis.summary,
        recordingDuration: duration,
        transcript: data.transcription
      };
      
      generateComprehensiveReport();
      
    } catch (error) {
      console.error('‚ùå Voice analysis failed:', error);
      videoDebugInfo.textContent = `Analysis failed: ${error.message}`;
    }
  }

  // ========== Report Generation ==========
  function generateComprehensiveReport() {
    const reportSection = document.getElementById('reportSection');
    const reportContent = document.getElementById('reportContent');
    
    const facialAverages = calculateFacialAverages();
    reportContent.innerHTML = createReportHTML(facialAverages);
    reportSection.style.display = 'block';
    reportSection.scrollIntoView({ behavior: 'smooth' });
    
    console.log("üìã Comprehensive report generated");
  }

  function calculateFacialAverages() {
    if (facialExpressionData.length === 0) {
      return {
        smile: 0,
        nervousness: 0,
        confidence: 0,
        engagement: 0,
        dominantEmotion: 'none',
        faceDetectionRate: 0
      };
    }
    
    const totals = facialExpressionData.reduce((acc, data) => {
      acc.smile += data.smile;
      acc.nervousness += data.nervousness;
      acc.confidence += data.confidence;
      acc.engagement += data.engagement;
      return acc;
    }, { smile: 0, nervousness: 0, confidence: 0, engagement: 0 });
    
    const count = facialExpressionData.length;
    
    const emotionCount = {};
    facialExpressionData.forEach(data => {
      emotionCount[data.dominantEmotion] = (emotionCount[data.dominantEmotion] || 0) + 1;
    });
    const dominantEmotion = Object.keys(emotionCount).reduce((a, b) => 
      emotionCount[a] > emotionCount[b] ? a : b
    );
    
    return {
      smile: totals.smile / count,
      nervousness: totals.nervousness / count,
      confidence: totals.confidence / count,
      engagement: totals.engagement / count,
      dominantEmotion: dominantEmotion,
      faceDetectionRate: (videoAnalysisData.framesWithFace / videoAnalysisData.totalFrames) * 100
    };
  }

  function createReportHTML(facialAverages) {
    const voice = voiceAnalysisData;
    
    return `
      <div class="report-header text-center mb-4">
        <h3 class="text-gold">üéØ Presentation Analysis Report</h3>
        <p class="text-muted">Duration: ${Math.round(voice.recordingDuration)} seconds</p>
      </div>
      
      <div class="row mb-4">
        <div class="col-md-6">
          <div class="metrics-card">
            <h5><i class="bi bi-emoji-smile"></i> Facial Expression Analysis</h5>
            <div class="score-grid">
              <div class="score-card">
                <div class="score-number">${Math.round(facialAverages.smile)}%</div>
                <div class="score-label">Smile Intensity</div>
              </div>
              <div class="score-card">
                <div class="score-number">${Math.round(facialAverages.confidence)}%</div>
                <div class="score-label">Confidence</div>
              </div>
              <div class="score-card">
                <div class="score-number">${Math.round(facialAverages.nervousness)}%</div>
                <div class="score-label">Nervousness</div>
              </div>
              <div class="score-card">
                <div class="score-number">${Math.round(facialAverages.engagement)}%</div>
                <div class="score-label">Engagement</div>
              </div>
            </div>
            <div class="mt-3">
              <small class="text-gold">Dominant Emotion:</small>
              <div class="fw-bold">${facialAverages.dominantEmotion.charAt(0).toUpperCase() + facialAverages.dominantEmotion.slice(1)}</div>
            </div>
          </div>
        </div>
        
        <div class="col-md-6">
          <div class="metrics-card">
            <h5><i class="bi bi-mic"></i> Voice Analysis</h5>
            <div class="score-grid">
              <div class="score-card">
                <div class="score-number">${Math.round(voice.wpm)}</div>
                <div class="score-label">Words/Min</div>
              </div>
              <div class="score-card">
                <div class="score-number">${voice.filler_count}</div>
                <div class="score-label">Filler Words</div>
              </div>
              <div class="score-card">
                <div class="score-number">${Math.round(voice.speech_clarity * 100)}%</div>
                <div class="score-label">Clarity</div>
              </div>
              <div class="score-card">
                <div class="score-number">${Math.round(voice.pace_score * 100)}%</div>
                <div class="score-label">Pace Score</div>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div class="metrics-card">
        <h5><i class="bi bi-lightbulb"></i> Personalized Suggestions</h5>
        <div class="suggestions-container">
          ${generateCombinedSuggestions(facialAverages, voice)}
        </div>
      </div>
      
      <div class="metrics-card">
        <h5><i class="bi bi-chat-left-text"></i> Transcript Preview</h5>
        <div class="transcript-preview">
          ${voice.transcript.substring(0, 300)}${voice.transcript.length > 300 ? '...' : ''}
        </div>
      </div>
      
      <div class="text-center mt-4">
        <button class="btn btn-record" onclick="downloadFullReport()">
          <i class="bi bi-download"></i> Download Full Report
        </button>
      </div>
    `;
  }

  function generateCombinedSuggestions(facial, voice) {
    const suggestions = [];
    
    if (facial.smile < 30) {
      suggestions.push("üòä <strong>Smile more:</strong> Try to incorporate more natural smiles to appear more approachable and engaging.");
    }
    
    if (facial.confidence < 60) {
      suggestions.push("üí™ <strong>Build confidence:</strong> Practice maintaining eye contact and using open body language to appear more confident.");
    }
    
    if (facial.nervousness > 40) {
      suggestions.push("üòå <strong>Reduce nervousness:</strong> Practice breathing exercises before presenting to help manage anxiety.");
    }
    
    if (facial.engagement < 50) {
      suggestions.push("üéØ <strong>Increase engagement:</strong> Use more facial expressions and vary your tone to keep audience engaged.");
    }
    
    if (voice.wpm < 120) {
      suggestions.push("üó£Ô∏è <strong>Increase speaking pace:</strong> Your speech is quite slow. Aim for 130-160 words per minute for better engagement.");
    } else if (voice.wpm > 180) {
      suggestions.push("üê¢ <strong>Slow down slightly:</strong> You're speaking very fast. Slow down to improve clarity and comprehension.");
    }
    
    if (voice.filler_count > 10) {
      suggestions.push("üö´ <strong>Reduce filler words:</strong> Practice pausing instead of using 'um', 'uh', or 'like' to sound more professional.");
    }
    
    if (voice.speech_clarity < 0.7) {
      suggestions.push("üéØ <strong>Improve clarity:</strong> Focus on enunciating words clearly and using pauses effectively.");
    }
    
    suggestions.push("üìä <strong>Practice regularly:</strong> Record yourself frequently to track improvement over time.");
    suggestions.push("üé≠ <strong>Balance expressions:</strong> Combine confident posture with warm facial expressions for maximum impact.");
    
    return suggestions.map(suggestion => `
      <div class="suggestion-item">
        <div class="suggestion-text">${suggestion}</div>
      </div>
    `).join('');
  }

  function downloadFullReport() {
    alert("üìÑ Full report download would be implemented here with PDF generation!");
  }

  // ========== Existing Audio Analysis Functions ==========
  async function analyzeAudio() {
    const file = audioFileInput.files[0];
    
    if (!file) {
      alert('Please select an audio file first');
      return;
    }
    
    console.log("üì§ Analyzing audio file:", file.name, "Size:", file.size);
    audioDebugInfo.textContent = `Selected: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)} MB)`;
    
    loadingSpinner.style.display = 'block';
    scoreGrid.style.display = 'none';
    detailedMetrics.style.display = 'none';
    audioAnalysisDebugInfo.textContent = 'Sending audio to server...';
    
    transcriptContainer.innerHTML = `
      <div class="text-center py-4">
        <div class="spinner-border text-gold" role="status">
          <span class="visually-hidden">Loading...</span>
        </div>
        <p class="mt-2">Analyzing your audio...</p>
      </div>
    `;
    
    try {
      const formData = new FormData();
      formData.append('audio', file, file.name);
      
      console.log("üì§ Sending audio to /api/analyze-audio...");
      
      const response = await fetch(`${API_BASE_URL}/api/analyze-audio`, {
        method: 'POST',
        body: formData
      });
      
      console.log("üì• Response status:", response.status);
      
      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(`Server error: ${response.status} - ${errorData.error || 'Unknown error'}`);
      }
      
      const data = await response.json();
      console.log("‚úÖ Analysis received:", data);
      audioAnalysisDebugInfo.textContent = 'Analysis completed successfully!';
      displayAnalysisResults(data);
      
    } catch (error) {
      console.error('‚ùå Backend error:', error);
      audioAnalysisDebugInfo.textContent = `Error: ${error.message}`;
      showError(`Analysis failed: ${error.message}`);
    } finally {
      loadingSpinner.style.display = 'none';
    }
  }

  function displayAnalysisResults(data) {
    if (data.status !== 'success') {
      showError(data.error || 'Analysis failed');
      return;
    }

    const { transcription, analysis } = data;
    const summary = analysis.summary;
    
    console.log("üìä Displaying analysis results:", summary);
    
    updateTranscript(analysis.buckets, transcription);
    updateScores(summary);
    updateDetailedMetrics(summary);
    
    showSuccessNotification(summary);
  }

  function updateTranscript(buckets, fullTranscript) {
    let transcriptHTML = '';
    
    if (buckets && buckets.length > 0) {
      buckets.forEach((bucket, index) => {
        const time = bucket.start ? formatTime(bucket.start) : `[${index + 1}]`;
        let text = bucket.text || '';
        
        const fillerWords = ["um", "uh", "you know", "i mean", "like", "so", "right"];
        fillerWords.forEach(filler => {
          const regex = new RegExp(`\\b${filler}\\b`, 'gi');
          text = text.replace(regex, `<span class="filler-word">${filler}</span>`);
        });
        
        transcriptHTML += `
          <div class="transcript-line">
            <span class="transcript-time">[${time}]</span> ${text}
          </div>
        `;
      });
    } else {
      transcriptHTML = `
        <div class="transcript-line">
          <span class="transcript-time">[Full Transcript]</span> ${fullTranscript}
        </div>
      `;
    }
    
    transcriptContainer.innerHTML = transcriptHTML;
  }

  function updateScores(summary) {
    document.getElementById('overallScore').textContent = Math.round(summary.final_score * 100) + '%';
    document.getElementById('clarityScore').textContent = Math.round(summary.speech_clarity * 100) + '%';
    document.getElementById('paceScore').textContent = Math.round(summary.pace_score * 100) + '%';
    document.getElementById('prosodyScore').textContent = Math.round(summary.prosody_score * 100) + '%';
    scoreGrid.style.display = 'grid';
  }

  function updateDetailedMetrics(summary) {
    document.getElementById('wordCount').textContent = summary.total_words || '--';
    document.getElementById('fillerCount').textContent = summary.filler_count || '--';
    document.getElementById('speakingRate').textContent = summary.wpm ? Math.round(summary.wpm) + ' WPM' : '--';
    document.getElementById('sentimentScore').textContent = summary.avg_sentiment_norm ? Math.round(summary.avg_sentiment_norm * 100) + '%' : '--';
    detailedMetrics.style.display = 'block';
  }

  function formatTime(seconds) {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs < 10 ? '0' : ''}${secs}`;
  }

  // ========== Notification Functions ==========
  function showSuccessNotification(summary) {
    const toast = document.createElement('div');
    toast.className = 'alert alert-success alert-dismissible fade show position-fixed';
    toast.style.cssText = 'top: 20px; right: 20px; z-index: 1050; min-width: 300px;';
    toast.innerHTML = `
      <strong><i class="bi bi-check-circle-fill"></i> Analysis Complete!</strong>
      <br>Overall Score: <strong>${Math.round(summary.final_score * 100)}%</strong>
      <button type="button" class="btn-close" data-bs-dismiss="alert"></button>
    `;
    document.body.appendChild(toast);
    
    setTimeout(() => {
      if (toast.parentNode) toast.remove();
    }, 5000);
  }

  function showError(message) {
    transcriptContainer.innerHTML = `
      <div class="text-center text-danger py-4">
        <i class="bi bi-exclamation-triangle-fill" style="font-size: 2rem;"></i>
        <p class="mt-2">${message}</p>
      </div>
    `;
  }

  // ========== API Test Function ==========
  async function testApi() {
    console.log("üîß Testing API connection...");
    
    try {
      const response = await fetch(`${API_BASE_URL}/api/test`);
      const data = await response.json();
      
      if (data.status === 'success') {
        console.log("‚úÖ API test successful:", data);
        alert(`‚úÖ API is working!\nWhisper loaded: ${data.whisper_loaded}\nMessage: ${data.message}`);
      } else {
        console.error("‚ùå API test failed:", data);
        alert(`‚ùå API test failed: ${data.message}`);
      }
    } catch (error) {
      console.error("‚ùå API test error:", error);
      alert(`‚ùå Cannot connect to API: ${error.message}\n\nMake sure Flask server is running on port 5000.`);
    }
  }

  // ========== Event Listeners ==========
  startRecordingButton.addEventListener('click', function() {
    if (isRecording) {
      stopCombinedRecording();
    } else {
      startCombinedRecording();
    }
  });

  testApiButton.addEventListener('click', testApi);

  analyzeAudioButton.addEventListener('click', analyzeAudio);

  audioFileInput.addEventListener('change', function() {
    const file = this.files[0];
    if (file) {
      audioDebugInfo.textContent = `Selected: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)} MB)`;
    } else {
      audioDebugInfo.textContent = 'No file selected';
    }
  });

  // ========== Initialize ==========
  console.log("üé§ AI Presentation Analyzer Ready");
  console.log("üí° Click 'Start Recording' to begin combined face & voice analysis");
  console.log(`üåê API Base URL: ${API_BASE_URL}`);
</script>
</body>
</html>